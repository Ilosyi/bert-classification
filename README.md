# BERT文本分类项目/大模型的端云协同

这是一个基于BERT模型的文本分类项目，用于对中文问答数据进行简单/复杂分类。

## 项目简介

本项目使用预训练的BERT模型（具体为`hfl/chinese-macbert-base`）对中文问答数据进行二分类任务。项目包含完整的数据处理、模型训练和保存功能。

## 功能特性

- 使用预训练的中文BERT模型进行文本分类
- 支持GPU加速训练
- 自动划分训练集和验证集
- 完整的训练日志和模型保存
- 支持在线和离线模型加载

## 环境要求

- Python 3.7+
- PyTorch 1.8+
- Transformers 4.0+
- CUDA（可选，用于GPU加速）

## 安装依赖

```bash
pip install torch transformers
```

## 项目结构

```
bert-classification/
├── train.py              # 主训练脚本
├── dataset/              # 数据集目录
│   └── simple_vs_complex_cn.json  # 中文问答数据集
├── results/              # 训练结果输出目录
├── logs/                 # 训练日志目录
└── macbert_classifier/   # 保存的模型目录
```

## 数据集格式

数据集应为JSON格式，包含以下字段：

```json
[
  {
    "question": "问题文本",
    "label": 0  // 标签：0表示简单，1表示复杂
  }
]
```

## 使用方法

### 1. 准备数据

将你的数据集文件放在`dataset/`目录下，确保格式正确。

### 2. 开始训练

```bash
python train.py
```

### 3. 训练参数

训练脚本包含以下主要参数：

- **模型**: `hfl/chinese-macbert-base`（中文MacBERT）
- **训练轮数**: 4 epochs
- **批次大小**: 2
- **学习率**: 默认
- **验证策略**: 每个epoch结束后评估
- **保存策略**: 每个epoch结束后保存模型

### 4. 输出文件

训练完成后，会在以下目录生成文件：

- `results/`: 训练过程中的检查点
- `logs/`: 训练日志
- `macbert_classifier/`: 最终保存的模型和分词器

## 模型说明

本项目使用的中文MacBERT模型具有以下特点：

- 基于BERT架构，针对中文优化
- 支持中文分词和语义理解
- 适合中文文本分类任务

## 训练配置

当前训练配置针对中等规模数据集优化：

- 使用GPU加速（如果可用）
- 自动处理内存不足的情况
- 支持断点续训
- 详细的训练日志记录

## 注意事项

1. 首次运行时会自动下载预训练模型，请确保网络连接正常
2. 如果GPU内存不足，可以调整`per_device_train_batch_size`参数
3. 训练时间取决于数据集大小和硬件配置

## 故障排除

### 常见问题

1. **模型下载失败**
   - 检查网络连接
   - 尝试使用离线缓存模式

2. **GPU内存不足**
   - 减小批次大小
   - 减少最大序列长度

3. **训练速度慢**
   - 确保使用GPU训练
   - 调整数据加载器的工作进程数

## 许可证

本项目采用MIT许可证。

## 贡献

欢迎提交Issue和Pull Request来改进项目。

## 联系方式

如有问题或建议，请通过GitHub Issues联系。 
